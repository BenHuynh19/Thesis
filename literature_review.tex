\section{Literature review}

\subsection{Pillars of modern asset pricing}
\subsubsection{Efficient market hypothesis}
The first brick of the modern financial foundations was laid-out by a French financial mathematician: Louis J.B.A Bachelier (1870-1946). Previously studied in mathematical physics, he turned his interest into analyzing the prices of warrants traded in the Paris stock market when he prepared for his PhD thesis under the famous mathematician Henry Poincare. He discovered, written in his thesis “Theory of speculation” in 1914, that the prices distributed randomly, and investors could never gain profits from the past price patterns, and he concluded “the mathematical expectation of the speculator was zero”. However, his revolutionary finding was not recognized until almost half a century later by the works of two authors, who will be Nobel Prize winners, a physicist-turned-economics Paul A. Samuelson and an Italian-American financial economist Eugene F. Fama. While Samuelson (1965) originally introduced the Martingale process\footnote{A variable X follows a Martingale model if and only if: $ E(X_{t+1} \mid X_0,X_1,...,X_t) = X_t $} , Fama (1965) followed an established Random walk model\footnote{A random walk model assumes that the steps, in which the variable takes each time away from their current value, are to be independently and identically distributed}  to arrive almost the same conclusion: the current traded price of an asset is already accounted all available information related to that asset. This is the heart of the Efficient Market Hypothesis (EMH) that we know today. Fama, though, coined the term efficient market in his writings (1965a and 1965b, respectively):
	“an “efficient” market for securities, that is, a market where, given the available information, actual prices at every point in time represent very good estimates of intrinsic values”, and:
“An “efficient” market is defined as a market where there are large numbers of rational, profit-maximizers actively competing, with each trying to predict future market values of individual securities, and where important current information is almost freely available to all participants. In an efficient market, on the average, competition will cause the full effects of new information on intrinsic values to be reflected “instantaneously” in actual prices”.
Lo (2017) acknowledged two layers of EMH sophistications from Fama’s works: “The Efficient Markets Hypothesis is a hypothesis about what information is available to market participants, and a second hypothesis about how prices fully reflect that information. The early tests of efficient markets focused on the what, evaluating which various types of information were or were not reflected in market prices. But the question of the how, the way markets actually incorporate information into prices is equally important— and much less obvious from the mathematics”. Unlike the laws of nature which are deterministically pre-defined such as quantum mechanics, general relativity, and so on, the EMH holds because of interactions among market participants. Each one of them will try to profit from even the least possible edge in historical information, and an army of players whose movements are to exploit instantly that inefficiency, will remove the profit opportunity and bring back the balance for the asset prices.
The success of EMH was widely, empirically proven throughout the following years such that Jensen (1978) regarded “there is no other proposition in economics which has more solid empirical evidence supporting it than the Efficient Market Hypothesis.”

\subsubsection{Capital Asset Pricing Model (CAPM)}
The other important pillar of the financial foundations was about viewing the trade-off between risk and reward. The latter was handily derived as the expected return of the asset/portfolio over the review period. The former, however, was subtler in measuring. It could be the total volatility of the asset’s returns (reflected through standard deviation) or downside risk (measured as the loss likelihood or the maximum drawdown). The relationship between risk and reward was drastically changed after William F. Sharpe published his work in 1964 (almost the same period of the EMH discovery). In his paper, he noticed that stock return variations could be divided into two components: the first element, he called “idiosyncratic”, fluctuations that were generated by unique aspects of the asset, such as a change in business direction, a present of a new innovative product in the industry, to name a few, and the second component, he referred as “systematic”, variations that were brought up by the general, market-wide circumstances such as unemployment rate, inflation, or oil shock. He further suggested that investors should be rewarded only for their systematic risk, not idiosyncratic one. In his reasoning, supported by mathematical formulations, since idiosyncratic risk was specifically isolated by individual assets, they could generally be eliminated by blending a substantial number of assets into a portfolio. Nonetheless, no matter how many assets an investor incorporated, the systematic risk, which was shared by all assets, would not be canceled out. This aspect was previously raised by will-be-another-Nobel-prize-winner Harry Markowitz in 1952: “This presumption, that the law of large numbers applies to a portfolio of securities, cannot be accepted. The returns from securities are too inter-correlated. Diversification cannot eliminate all variance.”
Therefore, a reward should be compensated for those who accepted this risk – it is nowadays known as a risk premium. This analysis, combined with an independent work of Lintner (1965), helped formulate the first formal model for asset pricing, hence the name Capital Asset Pricing Model (CAPM). The model essentially illustrated the relationship between systematic risk and expected return of an asset:\\
$ E(r_i)- R_f = \beta_i (R_m-R_f ) + \epsilon_i $\\
Where: $R_f$: risk-free rate, normally measured by the rate of 30 days to maturity Treasury Bill, $R_m$ is the expected return of the market portfolio, commonly represented by S&P500 index which is a basket of 500 largest capitalization values in the US market. And $\beta$, calculated as $ \frac{cov(r_i, R_m)}{\sigma^2_{R_m}} $, is the measure of systematic risk of the asset, the only relevant risk that investor should be rewarded for. Clearly shown from the model, an asset’ expected return is linearly proportional to its $\beta$. If an asset, for instance, has $\beta$ of 3, then it would have three times the systematic risk of the market portfolio, hence the expected return of that asset should be triple the risk premium of the market portfolio.
CAPM was a revolutionary idea that transformed the entire field of investing (indeed, in 1990, Sharpe was awarded a Nobel prize for this work, among other contributions). Not only it shed the new light into asset’s expected returns, but also helped provide a measure for portfolio performance. Specifically, Sharpe (1966), and Jensen (1968) leveraged the linear relationship to judge performance of a mutual fund manager by comparing directly excess return of the portfolio with the CAPM benchmark, called $\alpha$:\\
$ E(r_i)- r_f = \alpha_i+\beta_i(R_m-r_f) + \epsilon_i $\\
A manager added values to the portfolio if a positive alpha was found, meaning that the portfolio earned higher expected return then the level suggested by the CAPM, and this excess return explained the fees charged by the manger. 
A clear conclusion from the EMH was that no one could consistently predict the future movements of an asset’s prices, combining with the fact that only the systematic risk (as measured by $\beta$) would be rewarded, an entirely new investment vehicle was born: index funds. Based primarily upon direct results of both EMH and CAPM, an index fund would attempt to replicate the performance of an underlying market index. An investor, for instance, could trade an entire S&P500 by purchasing a S&P500 index fund with a small fee instead of individually collecting 500 different stocks into a portfolio. In other words, index funds promised to convey only $\beta$, no $\alpha$ at all. Because index funds did not try to delivery $\alpha$ (i.e., “beat the market), they’d require less resources such as talented professionals, computing power, and so on, hence significantly lower the fees they’d charge clients. Therefore, it helped provide an accessible investment channel for the general public to approximately gain expected returns as high as the market’s. And after its inception in the 1970s, index funds have blossomed into trillions of dollar industry thanks to its conveniences. 

\subsubsection{Multifactor models}
As the field evolved over time, some empirical results appeared to against the CAPM model. Specifically, Banz (1981) examined the monthly relationships between the market and common stock returns from 1926 to 1975, and pointed out that small firms (represented by low market capitalization) had higher returns than that of large firms, after controlling for the market risk. This size effect, as he called, was observed only for a set of very small stocks, and vanished when compared between medium and large stocks. To further investigate this anomaly – as CAPM claimed, Basu (1983) designed his test for traded stocks from 1962 – 1978 with a dual goal: how a firm’s earnings ratios (E/P) and size affected its returns? The finding, as he stated “common stock of high E/P firms earn, on average, higher risk-adjusted returns than the common stock of low E/P firms and that this effect is clearly significant even if experimental control is exercised over differences in firm size”, once again showed the incapability of CAMP in explaining various stock returns over the years. Rosenber et al. (1985) established two instrumental variables, book/price strategy (buy high book/price stocks, simultaneously sell low book/price stocks) and specific-return-reversal strategy (calculated disparity between actual and CAPM-fitted stock returns of the previous month), to argue that investors could, in principle, beat the market if they can “identify the valuation errors that correlate with these instruments”.
As a result of a series of empirical evidence opposed to the CAPM, Fama and French (1993) formally proposed a three-factor model (FF3) in an effort to capture stock’s expected returns related to both size and book-to-market ratios. They introduced the two new explanatory variables to the CAPM model, including SMB, measured as the return differences between small stocks less big stocks, and HML, as high B/M ratios minus low B/M ratios (they defined stocks with high B/M as value stocks since the market prices were closely reflected by the book values, not like stocks with low B/M, which were regarded as “growth” stock because these stock’s intrinsic values were immensely depended into their future prospects):\\
$ E(r_i)-R_f=\alpha_i+\beta_m(R_m-R_f)+\beta_sSMB+\beta_h HML+\epsilon_i $\\
Clearly seen from the model, if the coefficients, $\beta_m$, $\beta_s$ and $\beta_h$, fully reflected changes in expected returns, then the intercept $\alpha_i$ would be approximately zero. FF3 extended dependency of a stock/portfolio expected return on the size effect and value effect, not just the market fluctuations as the CAPM suggested. It, both theoretically and practically, made sense from the standpoint of risk considerations. Small firms and low book-to-market values should be logically risker than big firms and high book-to-market values, respectively. And because of that, investors should be rewarded for accepting these extra risks. This extension hugely succeeded in explaining the fittest of observed returns, hence opened up an entirely new field: active investment. Contrast to the index funds which were passively invested into the market portfolio, investors realized that if they could construct their portfolios based additionally on two other factor exposures from the FF3, the excess returns, after controlling for the market risk, would be realistically possible to earn.
Carhart (1997), motivated by the FF3’s inability of explain cross-sectional variation in momentum-sorted portfolio returns (Fama and French 1996), added on the fourth factor to extend the FF3 model: momentum. He defined this variable as “equal-weight average of firms with the highest 30 percent eleven-month returns lagged one month minus the equal-weight average of firms with the lowest 30 percent eleven-month returns lagged one month”. By applying the extended version of FF3 model on the monthly returns of mutual funds for the period January 1962 - December 1993, he revealed that “funds with higher returns last year have higher-than-average expected returns next year”. Apparently concluded from this study, investors would demand a risk premium for holding stocks that performed poorly in the previous year, regarding of their size, book-to-market value, and their systematic risks.
Investigating the relationship between abnormal stock returns and firm’s investments, Titman et al. (2004) empirically revealed the negative impact of capital expenditures and stock prices, stressing the price fluctuations around earning-announced events. The study applied a set of three tests on a large number of samples (1,635 firms a year for a period from July 1973 to June 1996). The article unveiled that those indicated factors (factors included size, book to market, momentums, and systematic risk) failed to explain the higher returns of low-level invested firms. In order words, the excess returns for holding stocks that spent less of their funds pursuing new investment opportunities was not associated with the mentioned factors. Obviously, investors unfavorably evaluated the news about firm’s enlargement, considered it as a sensitive stock to hold, and hence asked for a risk premium. In addition, Novy-Marx (2012) regressed monthly returns of stocks traded in the US market from July 1963 to December 2010, and revealed that profitable firms – measured by high ratios of gross profit-to-asset, earned higher returns compared to unprofitable firms. Investing in the latter case was deemed riskier (since they were considered as more likely to be insolvent, or less capable of maneuverability under difficult situations), and empirically showed that the investors were indeed compensated by this extra exposure. With primarily apparent evidence from these two studies, Fama and French (2015) re-constructed their FF3 model by augmenting the profitability and investment factors as below:\\
$ E(r_i)-R_f=\alpha_i+\beta_m(R_m-R_f)+\beta_sSMB+\beta_hHML+ \beta_pRMW+ \beta_iCMA+ \epsilon_i $ \\
Where RMW was calculated as the disparity between “robust minus weak” profitability, and CMA was the difference between low versus high investments in which they called conservative and aggressive, respectively. To empirically tested how well the 5 factor-model (FF5) performed, they sorted portfolios (according to the factors) comprised of all traded stocks in NYSE, Amex, and NASDAQ for the period from July 1963 to December 2013, and concluded that “the model explains between 71\% and 94\% of the cross-section variance of expected returns”. Surprisingly, FF5 – with their empirical success, did not incorporate the well-established momentum factor.
These multi-factor models, since then, have been widely expanded into a so-called “factor zoo” (a term introduced by Cochrane, 2011) with over a hundred of considered factors. For instance, Ibbotson et al (2013) studied the relative importance of liquidity (defined as a ratio of trading volume over outstanding shares) in stock’s long-term returns from 1972 to 2011. They constructed yearly-basis portfolios based upon 4 distinct factors (size, value, momentum and liquidity) over the review period, and pointed out that liquidity apparently differentiated portfolio returns likewise the other three widely accepted factors. Further strengthened the argument, the article treated liquidity as a series of returns in an attempt to linearly regress it with other style factors. It was shown that the monthly alphas (the intercept left over from the multivariate regressions) were all positive and significant, thus suggesting that liquidity may be used as a factor in modeling stock prices. Or another paper written by Chou et al (2019) suggested that there was a robust connection between asset growth (AG) and ex-ante stock returns. Specifically, they proposed and tested a hypothesis that AG acting as a guiding factor could help achieve superior and more consistent profitability than conventional factors such as value and size. These alike papers have contributed to a heating debate whether there exists a small, distinct set of factors that are statistically, practically suitable in asset pricing. And CAPM, FF3, FF5, combined with momentum seemed to be the shortlisted candidates for this matter.

\subsection{Background of this study}
\subsubsection{Efficient frontier}
Traditionally, investors selected each individual stock based completely upon their historical returns and fluctuations. Put it simply, if an investor fixed a certain level of return for an accepted risk, then a typical stock-collected process would be effortlessly proceeded by comparing whether a stock met that requirement. Mathematically speaking, this mechanism led to undesired results since it ignored the most foundational consideration: correlations among the chosen stocks. Markowitz (1952) presented a comprehensive framework, called mean variance optimization, precisely described how to construct a modern portfolio. Specifically, volatility, as he used covariance instead of individual standard deviations, was significantly reduced if we’d combine unrelated or better yet negatively correlated, stocks into a portfolio, and it would not affect the expected return. There were several assumptions underlying the theory, but two were prominent:
\begin{itemize}
	\item Investor’s preferences were fully captured by portfolio’s first two static moments: expected return and volatility
	\item Investors were rational, meaning that they would prefer a portfolio that had a lower level of risk given the same level of return, or vice versa, higher return for the same level of risk
\end{itemize}

\begin{figure}[h]

 \centering
 \includegraphics[scale=0.8]{fig/pic1_ef.png}
 \label{fig1:ef}
 \caption{Figure 1: Efficient frontier}

\end{figure}

Markowitz showed that there was a specific area where investors could combine different stocks with various weights to construct their investable portfolios, and it was bounded by the curved line as shown in the above chart. He graphically suggested that those orange dots were not efficient in a sense that investors could simply move vertically up (to earn higher returns with the same volatility), or move horizontally left (to reduce volatility with the same return). He called the black dot the MVP, “minimum-variance portfolio”, since there was no way one could get lower volatility than this portfolio. He further implied that investors would chose only the blue part of the curved line (started at the MVP), and named it the “efficient frontier”. Depending upon each investor’s preferences, they would move along the efficient frontier since it’d guarantee increased returns for an additional unit of volatility.

\subsubsection{Exchange traded funds (ETFs)}
An index fund, as depicted before, helped investors to mimic performance of the market portfolio by trading one ticker instead of a dozens of individual stocks. The natural questions arose, especially when CAPM was proved to be insufficient to capture excess returns adjusted for the market risk, in the financial market: Could we replicate performance of mutual funds and made it available to the market as a freely traded investment vehicle? And, exchange traded funds (ETFs) were born to answer precisely those questions. Essentially, an ETF provider, such as BlackRock or Vanguard, would examine broadly different assets and build a basket of them based upon certain criteria (for instance, tracking a specific sector, distinct size, value or momentum factors), just like a regular mutual fund but with much lower charged fees. They would offer this basket to the market with a specific ticker to trade, and investors can purchase a share of the basket, typically similar to buying shares of a company. Investors, then, could trade these ETF tickers openly on an exchange, considerably like a stock. However, diversification benefit was a hugely dominant advantage of an ETF compared to a regular stock as a result of the underlying ETF’s basket which comprised of multiple assets.
Broadly speaking, traditional ETFs were constructed based on capitalization-weighted design. Stocks with higher market capitalizations were weighted proportionally more than stocks with lower market capitalizations. Accordingly, these ETFs were mostly represented by a few of large stocks. To avoid this consolidation issue, “smart beta” indexes, referred to index funds or ETFs, were formulated with the objective of sharpening diversification or adjusting risk exposures. Smart beta, therefore, could be generally classified into two types as FTSE Russell\footnote{“The anatomy of smart beta”, https://content.ftserussell.com/sites/default/files/research/the-anatomy-of-smart-beta-final-1.pdf}  defined: 
\begin{itemize}
	\item Alternative to weighted indexes — typically designed to address perceived concentration risks in capitalization-weighted indexes or reduce volatility within the index; 
	\item Factor indexes — designed to replicate factor risk premiums in a transparent, rules-based and investable format
\end{itemize}

Put if differently, the first category was to increase the diversification benefits over capitalization-weighted index (mostly through combined different industries instead of selecting high valued stocks), and the latter was to capture excess returns recommended by a series of scholarly published papers about superior factors (such as size, value, momentum or volatility) in asset pricing literature. Not surprisingly, investors, then, may ask which one of these two strategies performed better than the other? And is that conclusion statistically significant? The results from this empirical study first suggests that factor ETFs (i.e., an efficient frontier combined of various factor ETFs) indeed outperformed sector ETFs (i.e., an efficient frontier mixed of diverse sector ETFs), and that conclusion was derived from a statistical test proposed by Basak, Jagannathan, and Sun (2002).


